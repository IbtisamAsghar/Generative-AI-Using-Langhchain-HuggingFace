{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95h5MYSqABLM",
        "outputId": "1569fd43-dd5b-4081-d0da-78c5f316cac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgVbq30XAko5"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi-ntD_nDE_A",
        "outputId": "8c527883-7276-4fdd-e424-972e874c6990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #  \"punkt\" tokenizer model, is required to split text into sentences.\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veoF4O8ODXXY"
      },
      "outputs": [],
      "source": [
        "corpus  = \"\"\"My name is Ibtisam I am the student of the bs Artificial Intelligence,\n",
        "I love to learn generative AI and build real worl projects.\n",
        "I am enjoying watching krish naik's course.\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8hZ5hSIDZA3",
        "outputId": "c2ded568-3645-41e9-ee14-a7e1f8bf48dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Ibtisam I am the student of the bs Artificial Intelligence,\n",
            "I love to learn generative AI and build real worl projects.\n",
            "I am enjoying watching krish naik's course.\n",
            " \n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0UuWWsuBEBP"
      },
      "outputs": [],
      "source": [
        "# Tokenization  --> paragraph to sentence\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgMARfyZBWaJ"
      },
      "outputs": [],
      "source": [
        "docs = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTHefwGDBYF9",
        "outputId": "5a89d8a5-45dc-46a5-fc0e-a6878b85db5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is Ibtisam I am the student of the bs Artificial Intelligence,\\nI love to learn generative AI and build real worl projects.',\n",
              " \"I am enjoying watching krish naik's course.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxn6HZpkEBvz",
        "outputId": "3e6fdff8-a454-41d2-9a17-f95708c6ac78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "type(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOY0t9COjaGE",
        "outputId": "4c91d482-d407-4191-a1bc-0a32baf0cb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHRIo6SnEDwT"
      },
      "outputs": [],
      "source": [
        "# paragraph to words\n",
        "# sentence to words\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cbd1p_nFHW_",
        "outputId": "75c9ee96-f5a5-48df-ea87-aa88904bf204"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Ibtisam',\n",
              " 'I',\n",
              " 'am',\n",
              " 'the',\n",
              " 'student',\n",
              " 'of',\n",
              " 'the',\n",
              " 'bs',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'generative',\n",
              " 'AI',\n",
              " 'and',\n",
              " 'build',\n",
              " 'real',\n",
              " 'worl',\n",
              " 'projects',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'enjoying',\n",
              " 'watching',\n",
              " 'krish',\n",
              " 'naik',\n",
              " \"'s\",\n",
              " 'course',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# paragraph to words\n",
        "# sentence to words\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "tB8thLd-iHkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xewtnEJNiTYN",
        "outputId": "13964301-5468-46a0-8afe-9bd081e212c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Ibtisam',\n",
              " 'I',\n",
              " 'am',\n",
              " 'the',\n",
              " 'student',\n",
              " 'of',\n",
              " 'the',\n",
              " 'bs',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'generative',\n",
              " 'AI',\n",
              " 'and',\n",
              " 'build',\n",
              " 'real',\n",
              " 'worl',\n",
              " 'projects',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'enjoying',\n",
              " 'watching',\n",
              " 'krish',\n",
              " 'naik',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in docs:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbEYFFgBiUip",
        "outputId": "0dce45ac-95be-4821-e0a2-142293fee43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'name', 'is', 'Ibtisam', 'I', 'am', 'the', 'student', 'of', 'the', 'bs', 'Artificial', 'Intelligence', ',', 'I', 'love', 'to', 'learn', 'generative', 'AI', 'and', 'build', 'real', 'worl', 'projects', '.']\n",
            "['I', 'am', 'enjoying', 'watching', 'krish', 'naik', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize # it make sure that the punctuation is also treated as the seperate world..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "MT43R5lsik15",
        "outputId": "71e66cc6-9cc4-492e-bc5c-6e5bca2920d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-2256708507>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m \u001b[0;31m# it make sure that the punctuation is also treated as the seperate world..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di6QWYJgkIB2",
        "outputId": "df3d77e8-bab7-4ef4-8c19-2b53d9ec0b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Ibtisam',\n",
              " 'I',\n",
              " 'am',\n",
              " 'the',\n",
              " 'student',\n",
              " 'of',\n",
              " 'the',\n",
              " 'bs',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'generative',\n",
              " 'AI',\n",
              " 'and',\n",
              " 'build',\n",
              " 'real',\n",
              " 'worl',\n",
              " 'projects',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'enjoying',\n",
              " 'watching',\n",
              " 'krish',\n",
              " 'naik',\n",
              " \"'\",\n",
              " 's',\n",
              " 'course',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "E928D674kLJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "c814Gm1kkVLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjU92Bl4kYJa",
        "outputId": "a842e475-51fc-432c-b154-c11d1a1418bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Ibtisam',\n",
              " 'I',\n",
              " 'am',\n",
              " 'the',\n",
              " 'student',\n",
              " 'of',\n",
              " 'the',\n",
              " 'bs',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'generative',\n",
              " 'AI',\n",
              " 'and',\n",
              " 'build',\n",
              " 'real',\n",
              " 'worl',\n",
              " 'projects.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'enjoying',\n",
              " 'watching',\n",
              " 'krish',\n",
              " 'naik',\n",
              " \"'s\",\n",
              " 'course',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X04NXCk4kfmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}